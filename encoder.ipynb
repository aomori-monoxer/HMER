{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LearnableFourierPositionalEncoding(nn.Module):\n",
    "    def __init__(self, G: int, M: int, F_dim: int, H_dim: int, D: int, gamma: float):\n",
    "        \"\"\"\n",
    "        Learnable Fourier Features from https://arxiv.org/pdf/2106.02795.pdf (Algorithm 1)\n",
    "        Implementation of Algorithm 1: Compute the Fourier feature positional encoding of a multi-dimensional position\n",
    "        Computes the positional encoding of a tensor of shape [N, G, M]\n",
    "        :param G: positional groups (positions in different groups are independent)\n",
    "        :param M: each point has a M-dimensional positional values\n",
    "        :param F_dim: depth of the Fourier feature dimension\n",
    "        :param H_dim: hidden layer dimension\n",
    "        :param D: positional encoding dimension\n",
    "        :param gamma: parameter to initialize Wr\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.M = M\n",
    "        self.F_dim = F_dim\n",
    "        self.H_dim = H_dim\n",
    "        self.D = D\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Projection matrix on learned lines (used in eq. 2)\n",
    "        self.Wr = nn.Linear(self.M, self.F_dim // 2, bias=False)\n",
    "        # MLP (GeLU(F @ W1 + B1) @ W2 + B2 (eq. 6)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.F_dim, self.H_dim, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.H_dim, self.D // self.G)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.Wr.weight.data, mean=0, std=self.gamma ** -2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Produce positional encodings from x\n",
    "        :param x: tensor of shape [N, G, M] that represents N positions where each position is in the shape of [G, M],\n",
    "                  where G is the positional group and each group has M-dimensional positional values.\n",
    "                  Positions in different positional groups are independent\n",
    "        :return: positional encoding for X\n",
    "        \"\"\"\n",
    "        N, G, M = x.shape\n",
    "        # Step 1. Compute Fourier features (eq. 2)\n",
    "        projected = self.Wr(x)\n",
    "        cosines = torch.cos(projected)\n",
    "        sines = torch.sin(projected)\n",
    "        F = 1 / np.sqrt(self.F_dim) * torch.cat([cosines, sines], dim=-1)\n",
    "        # Step 2. Compute projected Fourier features (eq. 6)\n",
    "        Y = self.mlp(F)\n",
    "        # Step 3. Reshape to x's shape\n",
    "        PEx = Y.reshape((N, self.D))\n",
    "        return PEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CROHMEDatamodule' from 'datamodule.datamodule' (/Users/aomori/repos/strokeAtt/datamodule/datamodule.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d0/f1dhrk_14dg50w588_15bh_40000gn/T/ipykernel_8182/2064493735.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tgt_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpRateRecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/strokeAtt/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatamodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/strokeAtt/datamodule/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCROHMEDatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CROHMEDatamodule' from 'datamodule.datamodule' (/Users/aomori/repos/strokeAtt/datamodule/datamodule.py)"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from utils import ce_loss, to_tgt_output, ExpRateRecorder\n",
    "import datamodule\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "from torch.nn.modules.transformer import TransformerDecoder\n",
    "from datamodule import vocab\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class StrokeImgEmbed(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(StrokeImgEmbed, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(10816, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "    \n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "\n",
    "\n",
    "class StrokesTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                stroke_feature_dim: int,\n",
    "                stroke_pos_feature_dim: int,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 PAD_IDX,\n",
    "                 device=None,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1,):\n",
    "        self.device = device\n",
    "        super(StrokesTransformer, self).__init__()\n",
    "        self.PAD_IDX = PAD_IDX\n",
    "        self.transformer = nn.Transformer(\n",
    "            batch_first=True,\n",
    "            d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.stroke_feature_dim = stroke_feature_dim\n",
    "        self.stroke_pos_feature_dim = stroke_pos_feature_dim\n",
    "\n",
    "        self.stroke_img_cnn = StrokeImgEmbed(stroke_feature_dim)\n",
    "        self.positional_encoding_add = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "        self.box_pos_enc = LearnableFourierPositionalEncoding(4, 1, 32, 32, self.stroke_pos_feature_dim, 100)\n",
    "\n",
    "        self.src_embed = nn.Linear(self.stroke_feature_dim + self.stroke_pos_feature_dim, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "\n",
    "\n",
    "\n",
    "    def just_encode(self, strokeImgs: FloatTensor, strokePaddingMask: FloatTensor, positions: FloatTensor):\n",
    "        batch_size, stroke_num, h, w = strokeImgs.size()\n",
    "\n",
    "        assert batch_size == positions.size(0)\n",
    "        assert stroke_num == positions.size(1)\n",
    "        assert positions.size(2) == 4\n",
    "        # assert tgt.size(2) == self.tgt_vocab_size\n",
    "\n",
    "        stroke_feat = self.stroke_img_cnn(strokeImgs.view((batch_size * stroke_num, 1, h, w))).view(batch_size, stroke_num, self.stroke_feature_dim)\n",
    "        pos_feat = self.box_pos_enc(positions.view((batch_size * stroke_num, 4, 1))).view((batch_size, stroke_num, self.stroke_pos_feature_dim))\n",
    "        words = torch.concat([stroke_feat, pos_feat], dim=2)\n",
    "        \"\"\"\n",
    "        words: [batch_size, stroke_num, stroke_feature_dim + 4]\n",
    "        \"\"\"\n",
    "        src = self.src_embed(words)\n",
    "        \"\"\"\n",
    "        src: [batch_size, stroke_num, emb_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        memory = self.transformer.encoder(src, mask=None, src_key_padding_mask=strokePaddingMask)\n",
    "        return memory\n",
    "\n",
    "    def just_decode(self, memory: Tensor, tgt: Tensor, strokePaddingMask: Tensor):\n",
    "        tgt_emb = self.positional_encoding_add(self.tgt_tok_emb(tgt))\n",
    "        tgt_pad_mask = tgt == self.PAD_IDX\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt.size(1), self.device)\n",
    "\n",
    "        output = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask, memory_mask=None,\n",
    "                              tgt_key_padding_mask=tgt_pad_mask,\n",
    "                              memory_key_padding_mask=strokePaddingMask)\n",
    "        return self.generator(output)\n",
    "\n",
    "    def forward(self, strokeImgs: FloatTensor, strokePaddingMask: FloatTensor, positions: FloatTensor, tgt: FloatTensor):\n",
    "        \"\"\"\n",
    "        strokeImgs: [batch_size, stroke_num, standard_h, standard_w]\n",
    "        positions: [batch_size, stroke_num, 4(left, top, right, bottom)]\n",
    "        tgt: [batch_size, answer_length]\n",
    "        stroke_padding_mask: [batch_size, stroke_num]\n",
    "        \"\"\"\n",
    "\n",
    "        memory = self.just_encode(strokeImgs, strokePaddingMask, positions)\n",
    "        return self.just_decode(memory, tgt, strokePaddingMask)\n",
    "\n",
    "class StrokesTransformerAdapter(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                stroke_feature_dim: int,\n",
    "                stroke_pos_feature_dim: int,\n",
    "                num_encoder_layers: int,\n",
    "                num_decoder_layers: int,\n",
    "                emb_size: int,\n",
    "                nhead: int,\n",
    "                patience: int,\n",
    "                learning_rate: float,\n",
    "                dim_feedforward: int = 512,\n",
    "                dropout: float = 0.1,):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = StrokesTransformer(\n",
    "            stroke_feature_dim = stroke_feature_dim,\n",
    "            stroke_pos_feature_dim = stroke_pos_feature_dim,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            emb_size = emb_size,\n",
    "            nhead = nhead,\n",
    "            tgt_vocab_size = len(vocab),\n",
    "            PAD_IDX = vocab.PAD_IDX,\n",
    "            device=self.device,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.exprate_recorder = ExpRateRecorder()\n",
    "\n",
    "\n",
    "    def forward(self, strokeImgs: FloatTensor, strokePaddingMask: FloatTensor, positions: FloatTensor, tgt: FloatTensor):\n",
    "        return self.model(strokeImgs, strokePaddingMask, positions, tgt)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adadelta(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=1e-6,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "\n",
    "        reduce_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"max\",\n",
    "            factor=0.1,\n",
    "            patience=self.hparams.patience // self.trainer.check_val_every_n_epoch,\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": reduce_scheduler,\n",
    "            \"monitor\": \"val_ExpRate\",\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": self.trainer.check_val_every_n_epoch,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "\n",
    "    def training_step(self, batch: datamodule.Batch, _):\n",
    "        tgt, out = to_tgt_output(batch.wordLabels, self.device)\n",
    "        out_hat = self(batch.strokeImgs, batch.strokeMasks, batch.positions, tgt)\n",
    "\n",
    "        loss = ce_loss(out_hat, out)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch: datamodule.Batch, _):\n",
    "        tgt, out = to_tgt_output(batch.wordLabels, self.device)\n",
    "        out_hat = self(batch.strokeImgs, batch.strokeMasks, batch.positions, tgt)\n",
    "\n",
    "        loss = ce_loss(out_hat, out)\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        indeed_max_len = max([len(t) for t in batch.wordLabels])\n",
    "        pred = self.greedy(\n",
    "            batch, max_len=int(indeed_max_len * 1.5)\n",
    "        )\n",
    "\n",
    "        self.exprate_recorder(pred, batch.wordLabels[0])\n",
    "        self.log(\n",
    "            \"val_ExpRate\",\n",
    "            self.exprate_recorder,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def greedy(self, d: datamodule.Batch, max_len:int):\n",
    "        assert len(d) == 1\n",
    "        memory = self.model.just_encode(d.strokeImgs, d.strokeMasks, d.positions)\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(max_len):\n",
    "            seq = LongTensor([[vocab.SOS_IDX] + ret])\n",
    "            outs = self.model.just_decode(memory, seq, d.strokeMasks)\n",
    "            id = outs[0][i].argmax()\n",
    "            ret.append(id.item())\n",
    "            if id == vocab.EOS_IDX:\n",
    "                break\n",
    "        return vocab.indices2words(ret)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract data from: ['strokes/CROHME2011_train/**/*.npy'], with data size: 921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0 | model            | StrokesTransformer | 21.1 M\n",
      "1 | exprate_recorder | ExpRateRecorder    | 0     \n",
      "--------------------------------------------------------\n",
      "21.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.1 M    Total params\n",
      "84.296    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract data from: ['strokes/CROHME2019_testGT/**/*.npy'], with data size: 1199\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1315 [00:00<?, ?it/s]                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 44/1315 [00:32<15:44,  1.35it/s, loss=3.42, v_num=42, val_loss=5.010, val_ExpRate=0.000]"
     ]
    }
   ],
   "source": [
    "import datamodule\n",
    "from importlib import reload\n",
    "reload(datamodule)\n",
    "import datamodule\n",
    "\n",
    "\n",
    "dm = datamodule.StrokeDatamodule([\"strokes/CROHME2011_train/**/*.npy\"], [\"strokes/CROHME2019_testGT/**/*.npy\"])\n",
    "import pytorch_lightning as pl\n",
    "from datamodule import vocab\n",
    "\n",
    "trainer = pl.Trainer(gpus=0,max_epochs=1)\n",
    "model = StrokesTransformerAdapter(**{\n",
    "    \"stroke_feature_dim\": 30, \n",
    "    \"stroke_pos_feature_dim\": 128, \n",
    "    \"num_encoder_layers\": 4,\n",
    "    \"num_decoder_layers\": 5, \n",
    "    \"emb_size\": 512, \n",
    "    \"nhead\": 8, \n",
    "    \"learning_rate\":0.3,\n",
    "    \"patience\": 20,\n",
    "})\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "969ad018a3902a35ffd10cacd07e0351d3ab7e4d0f81dff6dd800b4b68586ab3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('3.7.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
